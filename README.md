<details><summary>目录</summary><p>

- [动手学习强化学习](#动手学习强化学习)
- [初探强化学习](#初探强化学习)
  - [什么是强化学习](#什么是强化学习)
  - [强化学习的环境](#强化学习的环境)
  - [强化学习的目标](#强化学习的目标)
  - [强化学习中的数据](#强化学习中的数据)
  - [强化学习的独特性](#强化学习的独特性)
  - [小结](#小结)
- [MDP](#mdp)
  - [马尔可夫过程](#马尔可夫过程)
    - [随机过程](#随机过程)
    - [马尔可夫性质](#马尔可夫性质)
    - [马尔可夫过程](#马尔可夫过程-1)
  - [马尔可夫奖励过程](#马尔可夫奖励过程)
    - [回报](#回报)
    - [价值函数](#价值函数)
  - [马尔可夫决策过程](#马尔可夫决策过程)
    - [策略](#策略)
    - [状态价值函数](#状态价值函数)
    - [动作价值函数](#动作价值函数)
    - [贝尔曼期望方程](#贝尔曼期望方程)
  - [蒙特卡洛方法](#蒙特卡洛方法)
  - [占用度量](#占用度量)
  - [最优策略](#最优策略)
- [算法](#算法)
  - [动态规划算法](#动态规划算法)
  - [时序差分算法](#时序差分算法)
  - [Dyna-Q 算法](#dyna-q-算法)
  - [CartPole 环境](#cartpole-环境)
  - [DQN 算法](#dqn-算法)
    - [DQN 算法](#dqn-算法-1)
    - [DQN 改进算法](#dqn-改进算法)
  - [策略梯度算法](#策略梯度算法)
  - [Actor-Critic 算法](#actor-critic-算法)
  - [TRPO 算法](#trpo-算法)
  - [PPO 算法](#ppo-算法)
  - [DDPG 算法](#ddpg-算法)
  - [SAC 算法](#sac-算法)
</p></details><p></p>

# 动手学习强化学习

# 初探强化学习

* 序贯决策(sequential decision marking)
* 强化学习(reinforcement learning)

## 什么是强化学习

广泛地讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，
机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，
这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，
机器的目标是最大化在多轮交互过程中获得的累积奖励的期望。
强化学习用智能体（agent）这个概念来表示做决策的机器。相比于有监督学习中的“模型”，
强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，
而不只是给出一些预测信号。

这里，智能体有3种关键要素，即感知、决策和奖励。

* 感知。智能体在某种程度上感知环境的状态，从而知道自己所处的现状。
  例如，下围棋的智能体感知当前的棋盘情况；无人车感知周围道路的车辆、行人和红绿灯等情况；
  机器狗通过摄像头感知面前的图像，通过脚底的力学传感器来感知地面的摩擦功率和倾斜度等情况。
* 智能体根据当前的状态计算出达到目标需要采取的动作的过程叫作决策。例如，针对当前的棋盘决定下一颗落子的位置；
  针对当前的路况，无人车计算出方向盘的角度和刹车、油门的力度；针对当前收集到的视觉和力觉信号，
  机器狗给出4条腿的齿轮的角速度。策略是智能体最终体现出的智能形式，是不同智能体之间的核心区别。
* 奖励。环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。
  例如，围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶；机器狗是否在前进而没有摔倒。
  最大化累积奖励期望是智能体提升策略的目标，也是衡量智能体策略好坏的关键指标。

从以上分析可以看出，面向决策任务的强化学习和面向预测任务的有监督学习在形式上是有不少区别的。

* 首先，决策任务往往涉及多轮交互，即序贯决策；而预测任务总是单轮的独立任务。
  如果决策也是单轮的，那么它可以转化为“判别最优动作”的预测任务。
* 其次，因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，
  所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。

## 强化学习的环境

强化学习的智能体是在和一个动态环境的交互中完成序贯决策的。我们说一个环境是动态的，
意思就是它会随着某些因素的变化而不断演变，这在数学和物理中往往用随机过程来刻画。
对于一个随机过程，其最关键的要素就是状态以及状态转移的条件概率分布。

如果在环境这样一个自身演变的随机过程中加入一个外来的干扰因素，即智能体的动作，
那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定，用最简单的数学公式表示则是

$$\text{下一状态}\sim P(\cdot|\text{当前状态}, \text{智能体的动作})$$

根据上式可知，智能体决策的动作作用到环境中，使得环境发生相应的状态改变，
而智能体接下来则需要在新的状态下进一步给出决策。

由此我们看到，与面向决策任务的智能体进行交互的环境是一个动态的随机过程，
其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：
一是智能体决策的动作的随机性，二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。
通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。

## 强化学习的目标

智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由实数标量来表示。
这个奖励信号一般是诠释当前状态或动作的好坏的及时反馈信号，好比在玩游戏的过程中某一个操作获得的分数值。
整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体 **回报（return）**，
好比一盘游戏最后的分数值。根据环境的动态性我们可以知道，即使环境和智能体策略不变，
智能体的初始状态也不变，智能体和环境交互产生的结果也很可能是不同的，对应获得的回报也会不同。
因此，在强化学习中，我们关注回报的期望，并将其定义为 **价值（value）**，这就是强化学习中智能体学习的优化目标。

## 强化学习中的数据

有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，
通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。
这里，训练数据集背后的数据分布是完全不变的。

在强化学习中，数据是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，
那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。
因此，智能体的策略不同，与环境交互所产生的数据分布就不同

## 强化学习的独特性

一般的有监督学习任务：目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。
在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差(generalization error)

$$\text{最优模型} = arg \underset{模型}{max} \mathbb{E}_{(\text{特征, 标签})\sim \text{数据分布}}[损失函数(标签, 模型(特征))]$$

强化学习任务：最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。
策略的价值可以等价转换成奖励函数在策略的占用度量上的期望

$$\text{最优策略}= arg\underset{模型}{max}\mathbb{E}_{(\text{状态,动作})\sim\text{策略的占用度量}}[\text{奖励函数(状态, 动作)}]$$

观察以上两个优化公式，总结出两者的相似点和不同点。

* 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
* 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，
  即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，
  进而优化目标，即修改数据分布而目标函数不变。

综上所述，一般有监督学习和强化学习的范式之间的区别为：

* 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；
* 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，
  即最大化该分布下一个给定奖励函数的期望。

## 小结

在大多数情况下，强化学习任务往往比一般的有监督学习任务更难，因为一旦策略有所改变，
其交互产生的数据分布也会随之改变，并且这样的改变是高度复杂、不可追踪的，往往不能用显式的数学公式刻画。
这就好像一个混沌系统，我们无法得到其中一个初始设置对应的最终状态分布，而一般的有监督学习任务并没有这样的混沌效应。

# MDP

> Markov Decision process, MDP

强化学习中的 **环境** 一般就是一个马尔可夫决策过程(MDP)。
与多臂老虎机问题不同，马尔可夫决策过程包含 **状态信息** 以及 **状态之间的转移机制**。
如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，
也就是明确马尔可夫决策过程的各个组成要素。

## 马尔可夫过程

### 随机过程

在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $\mathcal{S}_{t}$ 表示，
所有可能的状态组成状态集合 $\mathcal{S}$。随机现象便是状态的变化过程。
在某时刻 $t$ 的状态 $\mathcal{S}_{t}$ 通常取决于 $t$ 时刻之前的状态。
我们将已知历史信息 $(\mathcal{S}_{1}, \cdots, \mathcal{S}_{t})$ 时下一个时刻状态 $\mathcal{S}_{t+1}$ 为的概率表示成 $P(\mathcal{S}_{t+1}|\mathcal{S}_{1}, \cdots, \mathcal{S}_{t})$。

### 马尔可夫性质

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property），
用公式表示为 $P(\mathcal{S}_{t+1}|\mathcal{S}_{t})=P(\mathcal{S}_{t+1}|\mathcal{S}_{1}, \cdots, \mathcal{S}_{t})$。
也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。
需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。
因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是时刻的状态其实包含了 $t-1$ 时刻的状态的信息，
通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，
所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

### 马尔可夫过程

马尔可夫过程（Markov process）指具有马尔可夫性质的随机过程，也被称为 **马尔可夫链（Markov chain）**。
通常用元组 $\langle\mathcal{S}, \mathcal{P}\rangle$ 描述一个马尔可夫过程，其中 $\mathcal{S}$ 是 **有限数量的状态集合**，
$\mathcal{P}$ 是 **状态转移矩阵(state transition matrix)**。

假设有 $n$ 个状态，此时 $\mathcal{S}=\{s_{1}, s_{2}, \cdots, s_{n}\}$，
状态转移矩阵 $\mathcal{P}$ 定义了所有状态对之间的转移概率，即

$$\mathcal{P}=\begin{bmatrix}
\mathcal{P(s_{1}|s_{1})} & \cdots & \mathcal{P(s_{n}|s_{1})} \\ 
\vdots                   & \ddots & \vdots                   \\
\mathcal{P(s_{1}|s_{n})} & \cdots & \mathcal{P(s_{n}|s_{n})}
\end{bmatrix}$$

矩阵 $\mathcal{P}$ 中第 $i$ 行，
第 $j$ 列元素 $P(s_{j}|s_{i})=P(\mathcal{S}_{t+1}=s_{j}|S_{t}=s_{i})$ 表示从状态 $s_{i}$ 转移到状态 $s_{j}$ 的概率，
称为 $\mathcal{P}(s^{'}|s)$ 为 **状态转移函数**。从某个状态出发，到达其他状态的概率和必须为 $1$，
即状态转移矩阵 $\mathcal{P}$ 的每一行的和为 $1$。

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），
这个步骤也被叫做采样（sampling）

## 马尔可夫奖励过程

> Markov Reward Process, MRP

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$，就可以得到 **马尔可夫奖励过程（Markov reward process）**。

一个马尔可夫奖励过程由 $\langle \mathcal{S}, \mathcal{P}, r, \gamma \rangle$ 构成，各个组成元素的含义如下：

* $\mathcal{S}$ 是有限状态的集合
* $\mathcal{P}$ 是状态转移矩阵
* $r$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望
* $\gamma$ 是折扣因子（discount factor），$\gamma$ 的取值范围为 $[0, 1)$。
  引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，
  所以我们需要对远期利益打一些折扣。接近 $1$ 的 $\gamma$ 更关注长期的累计奖励，
  接近 $0$ 的 $\gamma$ 更考虑短期奖励

### 回报

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $\mathcal{S}_{t}$ 开始，直到终止状态时，
所有奖励的衰减之和称为 **回报（Return）**，公式如下：

$$G_{t}=R_{t}+\gamma R_{t+1}+\gamma^{2}R_{t+2}+\cdots =\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}$$

其中，$R_{t}$ 表示在时刻 $t$ 获得的奖励。

### 价值函数

在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。
所有状态的价值就组成了价值函数（value function），价值函数的输入为某个状态，输出为这个状态的价值。
我们将价值函数写成 $V(s)=\mathbb{E}[G_{t}|S_{t}=s]$，展开为

$$\begin{align}
V(s)
&=\mathbb{E}[G_{t}|\mathcal{S}_{t}=s] \\
&=\mathbb{E}[R_{t} + \gamma R_{t+1} + \gamma^{2} R_{t+2} + \cdots|\mathcal{S}_{t}=s] \\
&=\mathbb{E}[R_{t} + \gamma (R_{t+1} + \gamma R_{t+2} + \cdots)|\mathcal{S}_{t}=s] \\
&=\mathbb{E}[R_{t} + \gamma G_{t+1}|\mathcal{S}_{t}=s] \\
&=\mathbb{E}[R_{t} + \gamma V(S_{t+1})|\mathcal{S}_{t}=s]
\end{align}$$

在上式的最后一个等号中：一方面，即时奖励的期望正是奖励函数的输出，即 $\mathbb{E}[R_{t}|\mathcal{S}_{t}=s]=r(s)$；
另一方面，等式中剩余部分 $\mathbb{E}[\gamma V(\mathcal{S}_{t+1})|\mathcal{S}_{t}=s]$ 可以根据从状态 $s$ 出发的转移概率得到，
即可以得到：$V(s) = r(s) + \gamma \sum_{s' \in \mathcal{S}} p(s'|s)V(s')$，
上式就是马尔可夫奖励过程中非常有名的 **贝尔曼方程（Bellman equation）**，对每一个状态都成立。

若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $\mathcal{S}=\{s_{1}, s_{2}, \cdots, s_{n}\}$，
我们将所有状态的价值表示成一个列向量 $\mathcal{V}=[V(s_{1}),V(s_{2}), \cdots, V(s_{n})]^{T}$，
同理，将奖励函数写成一个列向量 $\mathcal{R}=[r(s_{1}),r(s_{1}), \cdots, r(s_{1})]$。于是可以将贝尔曼方程写成矩阵的形式：

$$\mathcal{V}=\mathcal{R}+\gamma \mathcal{P}\mathcal{V}$$

$$
\begin{bmatrix}
V(s_{1}) \\
V(s_{2}) \\
\cdots   \\
V(s_{n}) \\
\end{bmatrix} =
\begin{bmatrix}
r(s_{1}) \\
r(s_{2}) \\
\cdots   \\
r(s_{n}) \\
\end{bmatrix} +
\gamma \begin{bmatrix}
P(s_{1}|s_{1}) & P(s_{2}|s_{1}) & \cdots P(s_{n}|s_{1}) \\
P(s_{1}|s_{2}) & P(s_{2}|s_{2}) & \cdots P(s_{n}|s_{2}) \\
\cdots \\
P(s_{1}|s_{n}) & P(s_{2}|s_{n}) & \cdots P(s_{n}|s_{n}) \\
\end{bmatrix}
\begin{bmatrix}
V(s_{1}) \\
V(s_{2}) \\
\cdots   \\
V(s_{n}) \\
\end{bmatrix}
$$

可以直接根据矩阵运算求解，得到以下解析解：

$$\mathcal{V}=\mathcal{R}+\gamma \mathcal{P}\mathcal{V}$$
$$(I-\gamma \mathcal{P})\mathcal{V} = \mathcal{R}$$
$$\mathcal{V} = (I-\gamma \mathcal{P})^{-1}\mathcal{R}$$

以上解析解的计算复杂度是 $O(n^{3})$，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。
求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用 **动态规划（dynamic programming）算法**、
**蒙特卡洛方法（Monte-Carlo method）** 和 **时序差分（temporal difference）**。

## 马尔可夫决策过程

> Markov Decision process, MDP

上面讨论到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，
就有了马尔可夫决策过程（Markov decision process，MDP）。我们将这个来自外界的刺激称为智能体（agent）的动作，
在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。

马尔可夫决策过程由元组 $\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \gamma\rangle$ 构成，其中：

* $\mathcal{S}$ 是状态的集合；
* $\mathcal{A}$ 是动作的集合；
* $\gamma$ 是折扣因子；
* $r(s, a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$，在奖励函数只取决于状态 $s$ 时，则退化为 $r(s)$；
* $\mathcal{P}(s'|s, a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s'$ 的概率。

不同于马尔可夫奖励过程，在马尔可夫决策过程中，通常存在一个智能体来执行动作。
马尔可夫决策过程是一个与时间相关的不断进行的过程，在智能体和环境 MDP 之间存在一个不断交互的过程。

一般而言，它们之间的交互是一个循环过程：智能体根据当前状态 $S_{t}$ 选择动作；
对于状态 $S_{t}$ 和动作 $A_{t}$，MDP 根据奖励函数和状态转移函数得到 $S_{t+1}$ 和 $R_{t}$ 并反馈给智能体。
智能体的目标是最大化得到的累计奖励。

智能体根据当前状态从动作的集合 $\mathcal{A}$ 中选择一个动作的函数，被称为 **策略**。

### 策略

智能体的策略（Policy）通常用字母 $\pi$ 表示。策略是 $\pi(a|s)=P(A_{t}=a|S_{t}=s)$ 一个函数，
表示在输入状态情况下采取动作的概率。

* 当一个策略是确定性策略（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0；
* 当一个策略是随机性策略（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。

### 状态价值函数

用 $V^{\pi}(s)$ 表示在 MDP 中基于策略 $\pi$ 的状态价值函数（state-value function），
定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报，数学表达为：

$$V^{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s]$$

### 动作价值函数

动作价值函数（action-value function）表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报：

$$Q^{\pi}(s, a)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s, A_{t}=a]$$

状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，
状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果：

$$V^{\pi}(s)=\sum_{a\in A}\pi(a|s)Q^{\pi}(s, a)$$

使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值，等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：

$$Q^{\pi}(s, a)=r(s,a)+\gamma\sum_{s' \in \mathcal{S}}P(s'|s, a)V^{\pi}(s')$$

### 贝尔曼期望方程

在贝尔曼方程中加上“期望”二字是为了与接下来的贝尔曼最优方程进行区分。
我们通过简单推导就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expectation Equation）：

贝尔曼方程：

$$
V(s)=\mathbb{E}[R_{t} + \gamma V(S_{t+1})|\mathcal{S}_{t}=s]
$$


$$\begin{align}
V^{\pi}
&=\mathbb{E}[R_{t}+\gamma V^{\pi}(S_{t+1})|S_{t}=s] \\
&=\sum_{a \in A}\pi (a|s)\Big(r(s,a)+\gamma \sum_{s' \in S} p(s'|s, a)V^{\pi}(s')\Big)
\end{align}$$

$$\begin{align}
Q^{\pi}
&=\mathbb{E}_{\pi}[R_{t}+\gamma Q^{\pi}(S_{t+1}, A_{t+1})|S_{t}=s, A_{t}=a] \\
&=r(s,a) + \gamma \sum_{s' \in S}p(s'|s,a)\sum_{a'\in A}\pi (a'|s')Q^{\pi}(s', a')
\end{align}$$

价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。

## 蒙特卡洛方法

## 占用度量

## 最优策略

# 算法

## 动态规划算法


## 时序差分算法


## Dyna-Q 算法



## CartPole 环境

* [openai gym CartPole v0](https://github.com/openai/gym/wiki/CartPole-v0)

## DQN 算法

### DQN 算法


### DQN 改进算法

## 策略梯度算法


## Actor-Critic 算法


## TRPO 算法


## PPO 算法

## DDPG 算法


## SAC 算法


